# dont-stop-pretraining
Implementing of the paper "Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks" (ACL 2020) in fast.ai and Huggingface transformers library.
